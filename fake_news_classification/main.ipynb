{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1600154387569",
   "display_name": "Python 3.7.8 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "import pandas as pd\n",
    "train = pd.read_csv('train.csv',index_col=0)\n",
    "train.head(3)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "    tid1  tid2                          title1_zh                  title2_zh  \\\nid                                                                             \n0      0     1      2017养老保险又新增两项，农村老人人人可申领，你领到了吗   警方辟谣“鸟巢大会每人领5万” 仍有老人坚持进京   \n3      2     3  \"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港  深圳GDP首超香港？深圳统计局辟谣：只是差距在缩小   \n1      2     4  \"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港       GDP首超香港？深圳澄清：还差一点点……   \n\n                                            title1_en  \\\nid                                                      \n0   There are two new old-age insurance benefits f...   \n3   \"If you do not come to Shenzhen, sooner or lat...   \n1   \"If you do not come to Shenzhen, sooner or lat...   \n\n                                            title2_en      label  \nid                                                                \n0   Police disprove \"bird's nest congress each per...  unrelated  \n3   Shenzhen's GDP outstrips Hong Kong? Shenzhen S...  unrelated  \n1   The GDP overtopped Hong Kong? Shenzhen clarifi...  unrelated  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tid1</th>\n      <th>tid2</th>\n      <th>title1_zh</th>\n      <th>title2_zh</th>\n      <th>title1_en</th>\n      <th>title2_en</th>\n      <th>label</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1</td>\n      <td>2017养老保险又新增两项，农村老人人人可申领，你领到了吗</td>\n      <td>警方辟谣“鸟巢大会每人领5万” 仍有老人坚持进京</td>\n      <td>There are two new old-age insurance benefits f...</td>\n      <td>Police disprove \"bird's nest congress each per...</td>\n      <td>unrelated</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2</td>\n      <td>3</td>\n      <td>\"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港</td>\n      <td>深圳GDP首超香港？深圳统计局辟谣：只是差距在缩小</td>\n      <td>\"If you do not come to Shenzhen, sooner or lat...</td>\n      <td>Shenzhen's GDP outstrips Hong Kong? Shenzhen S...</td>\n      <td>unrelated</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>4</td>\n      <td>\"你不来深圳，早晚你儿子也要来\"，不出10年深圳人均GDP将超香港</td>\n      <td>GDP首超香港？深圳澄清：还差一点点……</td>\n      <td>\"If you do not come to Shenzhen, sooner or lat...</td>\n      <td>The GDP overtopped Hong Kong? Shenzhen clarifi...</td>\n      <td>unrelated</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 1
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                            title1_en  \\\nid                                                      \n0   There are two new old-age insurance benefits f...   \n3   \"If you do not come to Shenzhen, sooner or lat...   \n1   \"If you do not come to Shenzhen, sooner or lat...   \n\n                                            title2_en      label  \nid                                                                \n0   Police disprove \"bird's nest congress each per...  unrelated  \n3   Shenzhen's GDP outstrips Hong Kong? Shenzhen S...  unrelated  \n1   The GDP overtopped Hong Kong? Shenzhen clarifi...  unrelated  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>title1_en</th>\n      <th>title2_en</th>\n      <th>label</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>There are two new old-age insurance benefits f...</td>\n      <td>Police disprove \"bird's nest congress each per...</td>\n      <td>unrelated</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>\"If you do not come to Shenzhen, sooner or lat...</td>\n      <td>Shenzhen's GDP outstrips Hong Kong? Shenzhen S...</td>\n      <td>unrelated</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>\"If you do not come to Shenzhen, sooner or lat...</td>\n      <td>The GDP overtopped Hong Kong? Shenzhen clarifi...</td>\n      <td>unrelated</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "cols = ['title1_en','title2_en','label']\n",
    "train = train.loc[:, cols]\n",
    "train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "def en_tokenizer(text):\n",
    "    tokens = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                            title1_en  \\\nid                                                      \n0   There are two new old-age insurance benefits f...   \n3   \"If you do not come to Shenzhen, sooner or lat...   \n1   \"If you do not come to Shenzhen, sooner or lat...   \n2   \"If you do not come to Shenzhen, sooner or lat...   \n9   \"How to discriminate oil from gutter oil by me...   \n\n                                     title1_tokenized  \nid                                                     \n0   There are two new oldage insurance benefits fo...  \n3   If you do not come to Shenzhen sooner or later...  \n1   If you do not come to Shenzhen sooner or later...  \n2   If you do not come to Shenzhen sooner or later...  \n9   How to discriminate oil from gutter oil by mea...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>title1_en</th>\n      <th>title1_tokenized</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>There are two new old-age insurance benefits f...</td>\n      <td>There are two new oldage insurance benefits fo...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>\"If you do not come to Shenzhen, sooner or lat...</td>\n      <td>If you do not come to Shenzhen sooner or later...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>\"If you do not come to Shenzhen, sooner or lat...</td>\n      <td>If you do not come to Shenzhen sooner or later...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>\"If you do not come to Shenzhen, sooner or lat...</td>\n      <td>If you do not come to Shenzhen sooner or later...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>\"How to discriminate oil from gutter oil by me...</td>\n      <td>How to discriminate oil from gutter oil by mea...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "train['title1_tokenized'] = train.loc[:, 'title1_en'].apply(en_tokenizer)\n",
    "train['title2_tokenized'] = train.loc[:, 'title2_en'].apply(en_tokenizer)\n",
    "train.iloc[:, [0, 3]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                            title2_en  \\\nid                                                      \n0   Police disprove \"bird's nest congress each per...   \n3   Shenzhen's GDP outstrips Hong Kong? Shenzhen S...   \n1   The GDP overtopped Hong Kong? Shenzhen clarifi...   \n2   Shenzhen's GDP topped Hong Kong last year? She...   \n9   It took 30 years of cooking oil to know that o...   \n\n                                     title2_tokenized  \nid                                                     \n0   Police disprove birds nest congress each perso...  \n3   Shenzhens GDP outstrips Hong Kong Shenzhen Sta...  \n1   The GDP overtopped Hong Kong Shenzhen clarifie...  \n2   Shenzhens GDP topped Hong Kong last year Shenz...  \n9   It took 30 years of cooking oil to know that o...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>title2_en</th>\n      <th>title2_tokenized</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Police disprove \"bird's nest congress each per...</td>\n      <td>Police disprove birds nest congress each perso...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Shenzhen's GDP outstrips Hong Kong? Shenzhen S...</td>\n      <td>Shenzhens GDP outstrips Hong Kong Shenzhen Sta...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>The GDP overtopped Hong Kong? Shenzhen clarifi...</td>\n      <td>The GDP overtopped Hong Kong Shenzhen clarifie...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Shenzhen's GDP topped Hong Kong last year? She...</td>\n      <td>Shenzhens GDP topped Hong Kong last year Shenz...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>It took 30 years of cooking oil to know that o...</td>\n      <td>It took 30 years of cooking oil to know that o...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "train.iloc[:, [1, 4]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "MAX_NUM_WORDS = 35000\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(num_words=MAX_NUM_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(641104,)"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "corpus_x1 = train.title1_tokenized\n",
    "corpus_x2 = train.title2_tokenized\n",
    "corpus = pd.concat([corpus_x1, corpus_x2])\n",
    "corpus.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                                title\nid                                                   \n0   There are two new oldage insurance benefits fo...\n3   If you do not come to Shenzhen sooner or later...\n1   If you do not come to Shenzhen sooner or later...\n2   If you do not come to Shenzhen sooner or later...\n9   How to discriminate oil from gutter oil by mea...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>title</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>There are two new oldage insurance benefits fo...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>If you do not come to Shenzhen sooner or later...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>If you do not come to Shenzhen sooner or later...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>If you do not come to Shenzhen sooner or later...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>How to discriminate oil from gutter oil by mea...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "pd.DataFrame(corpus.iloc[:5], columns=['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "320552"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "x1_train = tokenizer.texts_to_sequences(corpus_x1)\n",
    "x2_train = tokenizer.texts_to_sequences(corpus_x2)\n",
    "len(x1_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[[90, 17, 58, 27, 1285, 561, 788, 12, 64, 30, 7, 87, 426, 20, 8, 410, 174]]"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "x1_train[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['there', 'are', 'two', 'new', 'oldage', 'insurance', 'benefits', 'for', 'old', 'people', 'in', 'rural', 'areas', 'have', 'you', 'got', 'them']\n"
    }
   ],
   "source": [
    "for seq in x1_train[:1]:\n",
    "    print([tokenizer.index_word[idx] for idx in seq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "17 [90, 17, 58, 27, 1285]  ...\n28 [112, 8, 21, 9, 194]  ...\n28 [112, 8, 21, 9, 194]  ...\n28 [112, 8, 21, 9, 194]  ...\n11 [33, 2, 4323, 371, 44]  ...\n28 [112, 8, 21, 9, 194]  ...\n13 [112, 8, 25, 2407, 8]  ...\n28 [112, 8, 21, 9, 194]  ...\n10 [1065, 1065, 37, 4, 2699]  ...\n11 [33, 2, 4323, 371, 44]  ...\n"
    }
   ],
   "source": [
    "for seq in x1_train[:10]:\n",
    "    print(len(seq),seq[:5],' ...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "401"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "max([len(seq) for seq in x1_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 300\n",
    "x1_train = keras.preprocessing.sequence.pad_sequences(x1_train, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "x2_train = keras.preprocessing.sequence.pad_sequences(x2_train, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "id\n0    unrelated\n3    unrelated\n1    unrelated\n2    unrelated\n9       agreed\nName: label, dtype: object"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "train.label[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "label_to_index = {'unrelated': 0, 'agreed': 1, 'disagreed': 2}\n",
    "y_train = train.label.apply(lambda x: label_to_index[x])\n",
    "y_train = np.asarray(y_train).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([0., 0., 0., 0., 1., 0., 0., 0., 0., 1.], dtype=float32)"
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "y_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[1., 0., 0.],\n       [1., 0., 0.],\n       [1., 0., 0.],\n       [1., 0., 0.],\n       [0., 1., 0.],\n       [1., 0., 0.],\n       [1., 0., 0.],\n       [1., 0., 0.],\n       [1., 0., 0.],\n       [0., 1., 0.]], dtype=float32)"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "y_train = keras.utils.to_categorical(y_train)\n",
    "y_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "VALIDATION_RATIO = 0.1\n",
    "RANDOM_STATE = 1000\n",
    "\n",
    "x1_train, x1_val, x2_train, x2_val, y_train, y_val =\\\n",
    "train_test_split(x1_train, x2_train, y_train, test_size=VALIDATION_RATIO, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Training Set\n----------\nx1_train: (288496, 300)\nx2_train: (288496, 300)\ny_train : (288496, 3)\n----------\nx1_val:   (32056, 300)\nx2_val:   (32056, 300)\ny_val :   (32056, 3)\n----------\nTest Set\n"
    }
   ],
   "source": [
    "print('Training Set')\n",
    "print('-' * 10)\n",
    "print(f'x1_train: {x1_train.shape}')\n",
    "print(f'x2_train: {x2_train.shape}')\n",
    "print(f'y_train : {y_train.shape}')\n",
    "print('-' * 10)\n",
    "print(f'x1_val:   {x1_val.shape}')\n",
    "print(f'x2_val:   {x2_val.shape}')\n",
    "print(f'y_val :   {y_val.shape}')\n",
    "print('-' * 10)\n",
    "print('Test Set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "News Title 0: \n[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0   55  358   16  410   59   97   76  410   97   76 3504\n    1  129    3  258 2374  355]\n\nNews Title 1: \n[    0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0     0     0     0     0     0     0\n     0     0     0     0     0     0    33     2   111   377   122     2\n 22860    26    58  3002    56   703    56  1098     1  2112     1   374]\n\nNews Title 2: \n[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0   59  154   43   21\n  112  215  191 1930   20 3105  733    7    4 4537 8973    6  184  191\n 1930   20    2  111   12   10]\n\n"
    }
   ],
   "source": [
    "for i, seq in enumerate(x1_train[:3]):\n",
    "    print(f'News Title {i}: ')\n",
    "    print(seq)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "News Title 0: \n['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'wang', 'fei', 'has', 'got', 'what', 'she', 's', 'got', 'she', 's', 'carrying', 'the', 'baby', 'of', 'tse', 'ting', 'feng']\n\nNews Title 1: \n['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'how', 'to', 'pay', 'less', 'money', 'to', 'baobixin', 'these', 'two', 'functions', 'as', 'close', 'as', 'possible', 'the', 'faster', 'the', 'better']\n\nNews Title 2: \n['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'what', 'should', 'i', 'do', 'if', 'my', 'family', 'members', 'have', 'committed', 'suicide', 'in', 'a', 'unit', 'dormitory', 'and', 'their', 'family', 'members', 'have', 'to', 'pay', 'for', 'it']\n\n"
    }
   ],
   "source": [
    "for i, seq in enumerate(x1_train[:3]):\n",
    "    print(f'News Title {i}: ')\n",
    "    print([tokenizer.index_word.get(idx, '') for idx in seq])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 3\n",
    "MAX_NUM_WORDS = 35000\n",
    "MAX_SEQUENCE_LENGTH = 300\n",
    "NUM_EMBEDDING_DIM = 256\n",
    "NUM_LSTM_UNITS = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.layers import Embedding, LSTM, concatenate, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "top_input = Input(shape=(MAX_SEQUENCE_LENGTH, ), dtype='int32')\n",
    "buttom_input = Input(shape=(MAX_SEQUENCE_LENGTH, ), dtype='int32')\n",
    "\n",
    "embedding_layer = Embedding(MAX_NUM_WORDS, NUM_EMBEDDING_DIM)\n",
    "top_embedding = embedding_layer(top_input)\n",
    "buttom_embedding = embedding_layer(buttom_input)\n",
    "\n",
    "lstm_layer = LSTM(NUM_LSTM_UNITS)\n",
    "top_output = lstm_layer(top_embedding)\n",
    "buttom_output = lstm_layer(buttom_embedding)\n",
    "\n",
    "merged = concatenate([top_output, buttom_output], axis=1)\n",
    "\n",
    "fc_layer = Dense(NUM_CLASSES, 'softmax')\n",
    "predictions = fc_layer(merged)\n",
    "\n",
    "model = Model(inputs=[top_input, buttom_input], outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"functional_1\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_1 (InputLayer)            [(None, 300)]        0                                            \n__________________________________________________________________________________________________\ninput_2 (InputLayer)            [(None, 300)]        0                                            \n__________________________________________________________________________________________________\nembedding (Embedding)           (None, 300, 256)     8960000     input_1[0][0]                    \n                                                                 input_2[0][0]                    \n__________________________________________________________________________________________________\nlstm (LSTM)                     (None, 128)          197120      embedding[0][0]                  \n                                                                 embedding[1][0]                  \n__________________________________________________________________________________________________\nconcatenate (Concatenate)       (None, 256)          0           lstm[0][0]                       \n                                                                 lstm[1][0]                       \n__________________________________________________________________________________________________\ndense (Dense)                   (None, 3)            771         concatenate[0][0]                \n==================================================================================================\nTotal params: 9,157,891\nTrainable params: 9,157,891\nNon-trainable params: 0\n__________________________________________________________________________________________________\n"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1024\n",
    "NUM_EPOCHS = 30\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=5, verbose=2, mode='max')\n",
    "checkpoint = ModelCheckpoint(filepath='model.{epoch:02d}.h5')\n",
    "\n",
    "history = model.fit(x=[x1_train, x2_train],\\\n",
    "    y=y_train,\\\n",
    "    batch_size=BATCH_SIZE,\\\n",
    "    epochs=NUM_EPOCHS,\\\n",
    "    validation_data=([x1_val, x2_val], y_val),\\\n",
    "    shuffle=True,\\\n",
    "    callbacks=[early_stopping, checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[9.9999964e-01, 3.5797012e-08, 4.0200655e-07],\n       [9.9947995e-01, 3.3637854e-05, 4.8651820e-04],\n       [9.9998701e-01, 9.5523528e-06, 3.4135503e-06],\n       [9.9990809e-01, 1.3599135e-06, 9.0639289e-05],\n       [9.9997616e-01, 7.7560124e-08, 2.3732495e-05],\n       [3.9104998e-01, 6.0886270e-01, 8.7410051e-05],\n       [9.9995720e-01, 1.2388745e-05, 3.0361736e-05],\n       [2.4140307e-01, 7.5855392e-01, 4.3067128e-05],\n       [4.0024039e-01, 5.9956914e-01, 1.9040714e-04],\n       [9.0300465e-01, 9.4931506e-02, 2.0638276e-03]], dtype=float32)"
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "test = pd.read_csv('test.csv',index_col=0)\n",
    "\n",
    "test['title1_tokenized'] = test.loc[:, 'title1_en'].apply(en_tokenizer)\n",
    "test['title2_tokenized'] = test.loc[:, 'title2_en'].apply(en_tokenizer)\n",
    "\n",
    "x1_test = tokenizer.texts_to_sequences(test.title1_tokenized)\n",
    "x2_test = tokenizer.texts_to_sequences(test.title2_tokenized)\n",
    "\n",
    "x1_test = keras.preprocessing.sequence.pad_sequences(x1_test, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "x2_test = keras.preprocessing.sequence.pad_sequences(x2_test, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "model = keras.models.load_model('model.08.h5')\n",
    "predictions = model.predict([x1_test, x2_test])\n",
    "\n",
    "predictions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(80126, 1)\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "         Category\nid               \n321187  unrelated\n321190  unrelated\n321189  unrelated\n321193  unrelated\n321191  unrelated",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Category</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>321187</th>\n      <td>unrelated</td>\n    </tr>\n    <tr>\n      <th>321190</th>\n      <td>unrelated</td>\n    </tr>\n    <tr>\n      <th>321189</th>\n      <td>unrelated</td>\n    </tr>\n    <tr>\n      <th>321193</th>\n      <td>unrelated</td>\n    </tr>\n    <tr>\n      <th>321191</th>\n      <td>unrelated</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "index_to_label = {v: k for k, v in label_to_index.items()}\n",
    "\n",
    "test['Category'] = [index_to_label[idx] for idx in np.argmax(predictions, axis=1)]\n",
    "\n",
    "submission = test.loc[:, ['Category']]\n",
    "# submission.columns = ['Id', 'Category']\n",
    "print(submission.shape)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv')"
   ]
  }
 ]
}