{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1600673383992",
   "display_name": "Python 3.7.8 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "from IPython.display import clear_output\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = 'nmt'\n",
    "en_vocab_file = os.path.join(output_dir, 'en_vocab')\n",
    "zh_vocab_file = os.path.join(output_dir, 'zh_vocab')\n",
    "checkpoint_path = os.path.join(output_dir, 'checkpoints')\n",
    "log_dir = os.path.join(output_dir, 'logs')\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "{Split('train'): ['newscommentary_v14',\n                  'wikititles_v1',\n                  'uncorpus_v1',\n                  'casia2015',\n                  'casict2011',\n                  'casict2015',\n                  'datum2015',\n                  'datum2017',\n                  'neu2017'],\n Split('validation'): ['newstest2018']}\n"
    }
   ],
   "source": [
    "tmp_builder = tfds.builder('wmt19_translate/zh-en')\n",
    "pprint(tmp_builder.subsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = tfds.translate.wmt.WmtConfig(\n",
    "    version='1.0.0',\n",
    "    language_pair=('zh', 'en'),\n",
    "    subsets={\n",
    "        tfds.Split.TRAIN: ['newscommentary_v14']\n",
    "    }\n",
    ")\n",
    "builder = tfds.builder('wmt_translate', config=config)\n",
    "builder.download_and_prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<PrefetchDataset shapes: ((), ()), types: (tf.string, tf.string)>\n<PrefetchDataset shapes: ((), ()), types: (tf.string, tf.string)>\n"
    }
   ],
   "source": [
    "train_examples, val_examples = builder.as_dataset(split=['train[:20%]', 'train[20%:21%]'], as_supervised=True)\n",
    "print(train_examples)\n",
    "print(val_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tf.Tensor(b'The fear is real and visceral, and politicians ignore it at their peril.', shape=(), dtype=string)\ntf.Tensor(b'\\xe8\\xbf\\x99\\xe7\\xa7\\x8d\\xe6\\x81\\x90\\xe6\\x83\\xa7\\xe6\\x98\\xaf\\xe7\\x9c\\x9f\\xe5\\xae\\x9e\\xe8\\x80\\x8c\\xe5\\x86\\x85\\xe5\\x9c\\xa8\\xe7\\x9a\\x84\\xe3\\x80\\x82 \\xe5\\xbf\\xbd\\xe8\\xa7\\x86\\xe5\\xae\\x83\\xe7\\x9a\\x84\\xe6\\x94\\xbf\\xe6\\xb2\\xbb\\xe5\\xae\\xb6\\xe4\\xbb\\xac\\xe5\\x89\\x8d\\xe9\\x80\\x94\\xe5\\xa0\\xaa\\xe5\\xbf\\xa7\\xe3\\x80\\x82', shape=(), dtype=string)\n----------\ntf.Tensor(b'In fact, the German political landscape needs nothing more than a truly liberal party, in the US sense of the word \\xe2\\x80\\x9cliberal\\xe2\\x80\\x9d \\xe2\\x80\\x93 a champion of the cause of individual freedom.', shape=(), dtype=string)\ntf.Tensor(b'\\xe4\\xba\\x8b\\xe5\\xae\\x9e\\xe4\\xb8\\x8a\\xef\\xbc\\x8c\\xe5\\xbe\\xb7\\xe5\\x9b\\xbd\\xe6\\x94\\xbf\\xe6\\xb2\\xbb\\xe5\\xb1\\x80\\xe5\\x8a\\xbf\\xe9\\x9c\\x80\\xe8\\xa6\\x81\\xe7\\x9a\\x84\\xe4\\xb8\\x8d\\xe8\\xbf\\x87\\xe6\\x98\\xaf\\xe4\\xb8\\x80\\xe4\\xb8\\xaa\\xe7\\xac\\xa6\\xe5\\x90\\x88\\xe7\\xbe\\x8e\\xe5\\x9b\\xbd\\xe6\\x89\\x80\\xe8\\xb0\\x93\\xe2\\x80\\x9c\\xe8\\x87\\xaa\\xe7\\x94\\xb1\\xe2\\x80\\x9d\\xe5\\xae\\x9a\\xe4\\xb9\\x89\\xe7\\x9a\\x84\\xe7\\x9c\\x9f\\xe6\\xad\\xa3\\xe7\\x9a\\x84\\xe8\\x87\\xaa\\xe7\\x94\\xb1\\xe5\\x85\\x9a\\xe6\\xb4\\xbe\\xef\\xbc\\x8c\\xe4\\xb9\\x9f\\xe5\\xb0\\xb1\\xe6\\x98\\xaf\\xe4\\xb8\\xaa\\xe4\\xba\\xba\\xe8\\x87\\xaa\\xe7\\x94\\xb1\\xe4\\xba\\x8b\\xe4\\xb8\\x9a\\xe7\\x9a\\x84\\xe5\\x80\\xa1\\xe5\\xaf\\xbc\\xe8\\x80\\x85\\xe3\\x80\\x82', shape=(), dtype=string)\n----------\ntf.Tensor(b'Shifting to renewable-energy sources will require enormous effort and major infrastructure investment.', shape=(), dtype=string)\ntf.Tensor(b'\\xe5\\xbf\\x85\\xe9\\xa1\\xbb\\xe4\\xbb\\x98\\xe5\\x87\\xba\\xe5\\xb7\\xa8\\xe5\\xa4\\xa7\\xe7\\x9a\\x84\\xe5\\x8a\\xaa\\xe5\\x8a\\x9b\\xe5\\x92\\x8c\\xe5\\x9f\\xba\\xe7\\xa1\\x80\\xe8\\xae\\xbe\\xe6\\x96\\xbd\\xe6\\x8a\\x95\\xe8\\xb5\\x84\\xe6\\x89\\x8d\\xe8\\x83\\xbd\\xe5\\xae\\x8c\\xe6\\x88\\x90\\xe5\\x90\\x91\\xe5\\x8f\\xaf\\xe5\\x86\\x8d\\xe7\\x94\\x9f\\xe8\\x83\\xbd\\xe6\\xba\\x90\\xe7\\x9a\\x84\\xe8\\xbf\\x87\\xe6\\xb8\\xa1\\xe3\\x80\\x82', shape=(), dtype=string)\n----------\n"
    }
   ],
   "source": [
    "for en, zh in train_examples.take(3):\n",
    "    print(en)\n",
    "    print(zh)\n",
    "    print('-' * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "The fear is real and visceral, and politicians ignore it at their peril.\n这种恐惧是真实而内在的。 忽视它的政治家们前途堪忧。\n----------\nIn fact, the German political landscape needs nothing more than a truly liberal party, in the US sense of the word “liberal” – a champion of the cause of individual freedom.\n事实上，德国政治局势需要的不过是一个符合美国所谓“自由”定义的真正的自由党派，也就是个人自由事业的倡导者。\n----------\nShifting to renewable-energy sources will require enormous effort and major infrastructure investment.\n必须付出巨大的努力和基础设施投资才能完成向可再生能源的过渡。\n----------\nIn this sense, it is critical to recognize the fundamental difference between “urban villages” and their rural counterparts.\n在这方面，关键在于认识到“城市村落”和农村村落之间的根本区别。\n----------\nA strong European voice, such as Nicolas Sarkozy’s during the French presidency of the EU, may make a difference, but only for six months, and at the cost of reinforcing other European countries’ nationalist feelings in reaction to the expression of “Gallic pride.”\n法国担任轮值主席国期间尼古拉·萨科奇统一的欧洲声音可能让人耳目一新，但这种声音却只持续了短短六个月，而且付出了让其他欧洲国家在面对“高卢人的骄傲”时民族主义情感进一步被激发的代价。\n----------\nMost of Japan’s bondholders are nationals (if not the central bank) and have an interest in political stability.\n日本债券持有人大多为本国国民（甚至中央银行 ） ， 政治稳定符合他们的利益。\n----------\nPaul Romer, one of the originators of new growth theory, has accused some leading names, including the Nobel laureate Robert Lucas, of what he calls “mathiness” – using math to obfuscate rather than clarify.\n新增长理论创始人之一的保罗·罗默（Paul Romer）也批评一些著名经济学家，包括诺贝尔奖获得者罗伯特·卢卡斯（Robert Lucas）在内，说他们“数学性 ” （ 罗默的用语）太重，结果是让问题变得更加模糊而不是更加清晰。\n----------\nIt is, in fact, a capsule depiction of the United States Federal Reserve and the European Central Bank.\n事实上，这就是对美联储和欧洲央行的简略描述。\n----------\nGiven these variables, the degree to which migration is affected by asylum-seekers will not be easy to predict or control.\n考虑到这些变量，移民受寻求庇护者的影响程度很难预测或控制。\n----------\nWASHINGTON, DC – In the 2016 American presidential election, Hillary Clinton and Donald Trump agreed that the US economy is suffering from dilapidated infrastructure, and both called for greater investment in renovating and upgrading the country’s public capital stock.\n华盛顿—在2016年美国总统选举中，希拉里·克林顿和唐纳德·特朗普都认为美国经济饱受基础设施陈旧的拖累，两人都要求加大投资用于修缮和升级美国公共资本存量。\n----------\n"
    }
   ],
   "source": [
    "sample_examples = []\n",
    "num_samples = 10\n",
    "\n",
    "for en_t, zh_t in train_examples.take(num_samples):\n",
    "    en = en_t.numpy().decode('utf-8')\n",
    "    zh = zh_t.numpy().decode('utf-8')\n",
    "\n",
    "    print(en)\n",
    "    print(zh)\n",
    "    print('-' * 10)\n",
    "\n",
    "    sample_examples.append((en, zh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Load builded corpus: nmt\\en_vocab\nSize of corpus: 8113\nFirst 10 subwords: [', ', 'the_', 'of_', 'to_', 'and_', 's_', 'in_', 'a_', 'is_', 'that_']\nWall time: 31.9 ms\n"
    }
   ],
   "source": [
    "%%time\n",
    "try:\n",
    "    subword_encoder_en = tfds.features.text.SubwordTextEncoder.load_from_file(en_vocab_file)\n",
    "    print(f'Load builded corpus: {en_vocab_file}')\n",
    "except:\n",
    "    print(f'Build corpus: {en_vocab_file}')\n",
    "    subword_encoder_en = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
    "        (en.numpy() for en, _ in train_examples),\n",
    "        target_vocab_size=2**13\n",
    "    )\n",
    "    subword_encoder_en.save_to_file(en_vocab_file)\n",
    "\n",
    "print(f'Size of corpus: {subword_encoder_en.vocab_size}')\n",
    "print(f'First 10 subwords: {subword_encoder_en.subwords[:10]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[3461, 7889, 9, 3502, 4379, 1134, 7903]\n['Taiwan', ' ', 'is ', 'bea', 'uti', 'ful', '.']\n"
    }
   ],
   "source": [
    "sample_string = 'Taiwan is beautiful.'\n",
    "indices = subword_encoder_en.encode(sample_string)\n",
    "print(indices)\n",
    "print([subword_encoder_en.decode([idx]) for idx in indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Load builded corpus: nmt\\zh_vocab\nSize of corpus: 4205\nFirst 10 subwords: ['的', '，', '。', '国', '在', '是', '一', '和', '不', '这']\nWall time: 17 ms\n"
    }
   ],
   "source": [
    "%%time\n",
    "try:\n",
    "    subword_encoder_zh = tfds.features.text.SubwordTextEncoder.load_from_file(zh_vocab_file)\n",
    "    print(f'Load builded corpus: {zh_vocab_file}')\n",
    "except:\n",
    "    print(f'Build corpus: {zh_vocab_file}')\n",
    "    subword_encoder_zh = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
    "        (zh.numpy() for _, zh in train_examples),\n",
    "        target_vocab_size=2**13,\n",
    "        max_subword_length=1\n",
    "    )\n",
    "    subword_encoder_zh.save_to_file(zh_vocab_file)\n",
    "\n",
    "print(f'Size of corpus: {subword_encoder_zh.vocab_size}')\n",
    "print(f'First 10 subwords: {subword_encoder_zh.subwords[:10]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(en_t, zh_t):\n",
    "    en_vocab_size = subword_encoder_en.vocab_size\n",
    "    zh_vocab_size = subword_encoder_zh.vocab_size\n",
    "    en_indices = [en_vocab_size] + subword_encoder_en.encode(en_t.numpy()) + [en_vocab_size + 1]\n",
    "    zh_indices = [zh_vocab_size] + subword_encoder_zh.encode(zh_t.numpy()) + [zh_vocab_size + 1]\n",
    "    return en_indices, zh_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Index of BOS in en:  8113\nIndex of EOS in en:  8114\nIndex of BOS in zh:  4205\nIndex of EOS in zh:  4206\n\nTensor:\n(<tf.Tensor: shape=(), dtype=string, numpy=b'The fear is real and visceral, and politicians ignore it at their peril.'>,\n <tf.Tensor: shape=(), dtype=string, numpy=b'\\xe8\\xbf\\x99\\xe7\\xa7\\x8d\\xe6\\x81\\x90\\xe6\\x83\\xa7\\xe6\\x98\\xaf\\xe7\\x9c\\x9f\\xe5\\xae\\x9e\\xe8\\x80\\x8c\\xe5\\x86\\x85\\xe5\\x9c\\xa8\\xe7\\x9a\\x84\\xe3\\x80\\x82 \\xe5\\xbf\\xbd\\xe8\\xa7\\x86\\xe5\\xae\\x83\\xe7\\x9a\\x84\\xe6\\x94\\xbf\\xe6\\xb2\\xbb\\xe5\\xae\\xb6\\xe4\\xbb\\xac\\xe5\\x89\\x8d\\xe9\\x80\\x94\\xe5\\xa0\\xaa\\xe5\\xbf\\xa7\\xe3\\x80\\x82'>)\n---------------\nIndices:\n([8113, 16, 1284, 9, 243, 5, 1275, 1756, 156, 1, 5, 1016, 5566, 21, 38, 33, 2982, 7965, 7903, 8114], [4205, 10, 151, 574, 1298, 6, 374, 55, 29, 193, 5, 1, 3, 3981, 931, 431, 125, 1, 17, 124, 33, 20, 97, 1089, 1247, 861, 3, 4206])\n"
    }
   ],
   "source": [
    "en_t, zh_t = next(iter(train_examples))\n",
    "en_indices, zh_indices = encode(en_t, zh_t)\n",
    "print('Index of BOS in en: ', subword_encoder_en.vocab_size)\n",
    "print('Index of EOS in en: ', subword_encoder_en.vocab_size + 1)\n",
    "print('Index of BOS in zh: ', subword_encoder_zh.vocab_size)\n",
    "print('Index of EOS in zh: ', subword_encoder_zh.vocab_size + 1)\n",
    "\n",
    "print('\\nTensor:')\n",
    "pprint((en_t, zh_t))\n",
    "print('-' * 15)\n",
    "print('Indices:')\n",
    "print((en_indices, zh_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tf.Tensor(\n[8113   16 1284    9  243    5 1275 1756  156    1    5 1016 5566   21\n   38   33 2982 7965 7903 8114], shape=(20,), dtype=int64)\ntf.Tensor(\n[4205   10  151  574 1298    6  374   55   29  193    5    1    3 3981\n  931  431  125    1   17  124   33   20   97 1089 1247  861    3 4206], shape=(28,), dtype=int64)\n"
    }
   ],
   "source": [
    "tf_encode = lambda en_t, zh_t: tf.py_function(encode, [en_t, zh_t], [tf.int64, tf.int64])\n",
    "\n",
    "tmp_dataset = train_examples.map(tf_encode)\n",
    "en_indices, zh_indices = next(iter(tmp_dataset))\n",
    "print(en_indices)\n",
    "print(zh_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 100\n",
    "\n",
    "def filter_max_length(en, zh, max_length = MAX_LENGTH):\n",
    "    return tf.logical_and(tf.size(en) <= max_length, tf.size(zh) <= max_length)\n",
    "tmp_dataset = tmp_dataset.filter(filter_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "en_batch:\ntf.Tensor(\n[[8113   16 1284 ...    0    0    0]\n [8113   44  369 ...    0    0    0]\n [8113 1894 1302 ...    0    0    0]\n ...\n [8113 1809 5706 ...    0    0    0]\n [8113 1634    1 ...    0    0    0]\n [8113  100 2542 ...    0    0    0]], shape=(64, 71), dtype=int64)\n---------------\nzh_batch:\ntf.Tensor(\n[[4205   10  151 ...    0    0    0]\n [4205  109   55 ...    0    0    0]\n [4205  206  275 ...    0    0    0]\n ...\n [4205    9  270 ...    0    0    0]\n [4205  327  363 ...    0    0    0]\n [4205   16    4 ...    0    0    0]], shape=(64, 99), dtype=int64)\n"
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "tmp_dataset = tmp_dataset.padded_batch(BATCH_SIZE, padded_shapes=([-1], [-1]))\n",
    "en_batch, zh_batch = next(iter(tmp_dataset))\n",
    "print('en_batch:')\n",
    "print(en_batch)\n",
    "print('-' * 15)\n",
    "print('zh_batch:')\n",
    "print(zh_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 100\n",
    "BATCH_SIZE = 128\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "train_dataset = (train_examples\n",
    "                 .map(tf_encode)\n",
    "                 .filter(filter_max_length)\n",
    "                 .cache()\n",
    "                 .shuffle(BUFFER_SIZE)\n",
    "                 .padded_batch(BATCH_SIZE, padded_shapes=(100, 100))\n",
    "                 .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "\n",
    "val_dataset = (val_examples\n",
    "               .map(tf_encode)\n",
    "               .filter(filter_max_length)\n",
    "               .cache()\n",
    "               .shuffle(BUFFER_SIZE)\n",
    "               .padded_batch(BATCH_SIZE, padded_shapes=(100, 100))\n",
    "               .prefetch(tf.data.experimental.AUTOTUNE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tf.Tensor(\n[[8113 1633   24 ...    0    0    0]\n [8113  100   24 ...    0    0    0]\n [8113  559 1063 ...    0    0    0]\n ...\n [8113  214  911 ...    0    0    0]\n [8113 3509   11 ...    0    0    0]\n [8113   87 4462 ...    0    0    0]], shape=(128, 100), dtype=int64)\ntf.Tensor(\n[[4205  699  178 ...    0    0    0]\n [4205   29   16 ...    0    0    0]\n [4205   81  286 ...    0    0    0]\n ...\n [4205   17  123 ...    0    0    0]\n [4205   10    6 ...    0    0    0]\n [4205  241   53 ...    0    0    0]], shape=(128, 100), dtype=int64)\n"
    }
   ],
   "source": [
    "en_batch, zh_batch = next(iter(train_dataset))\n",
    "print(en_batch)\n",
    "print(zh_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.logical_not(tf.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_mean(loss_)\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "val_loss = tf.keras.metrics.Mean(name='val_loss')\n",
    "val_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='val_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "input_vocab_size: 8115\ntarget_vocab_size: 4207\n"
    }
   ],
   "source": [
    "num_layers = 4 \n",
    "d_model = 128\n",
    "dff = 512\n",
    "num_heads = 8\n",
    "\n",
    "input_vocab_size = subword_encoder_en.vocab_size + 2\n",
    "target_vocab_size = subword_encoder_zh.vocab_size + 2\n",
    "dropout_rate = 0.1\n",
    "\n",
    "print(\"input_vocab_size:\", input_vocab_size)\n",
    "print(\"target_vocab_size:\", target_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from learningrate import CustomSchedule\n",
    "\n",
    "learning_rate = CustomSchedule(d_model)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "這個 Transformer 有 4 層 Encoder / Decoder layers\nd_model: 128\nnum_heads: 8\ndff: 512\ninput_vocab_size: 8115\ntarget_vocab_size: 4207\ndropout_rate: 0.1\n\n"
    }
   ],
   "source": [
    "from model import Transformer\n",
    "\n",
    "transformer = Transformer(num_layers, d_model, num_heads, dff,\n",
    "                          input_vocab_size, target_vocab_size, dropout_rate)\n",
    "\n",
    "print(f\"\"\"這個 Transformer 有 {num_layers} 層 Encoder / Decoder layers\n",
    "d_model: {d_model}\n",
    "num_heads: {num_heads}\n",
    "dff: {dff}\n",
    "input_vocab_size: {input_vocab_size}\n",
    "target_vocab_size: {target_vocab_size}\n",
    "dropout_rate: {dropout_rate}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask\n",
    "\n",
    "def create_padding_mask(seq):\n",
    "    mask = tf.cast(tf.equal(seq, 0), tf.float32)\n",
    "    return mask[:, tf.newaxis, tf.newaxis, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masks(inp, tar):\n",
    "    enc_padding_mask = create_padding_mask(inp)\n",
    "    dec_padding_mask = create_padding_mask(inp)\n",
    "\n",
    "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "    dec_target_padding_mask = create_padding_mask(tar)\n",
    "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "    \n",
    "    return enc_padding_mask, combined_mask, dec_padding_mask    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, tar):\n",
    "    tar_inp = tar[:, :-1]\n",
    "    tar_real = tar[:, 1:]\n",
    "\n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, _ = transformer(inp, tar_inp, True, enc_padding_mask, combined_mask, dec_padding_mask)\n",
    "        loss = loss_function(tar_real, predictions)\n",
    "    \n",
    "    gradients = tape.gradient(loss, transformer.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(tar_real, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def val_step(inp, tar):\n",
    "    tar_inp = tar[:, :-1]\n",
    "    tar_real = tar[:, 1:]\n",
    "\n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
    "\n",
    "    predictions, _ = transformer(inp, tar_inp, False, enc_padding_mask, combined_mask, dec_padding_mask)\n",
    "    loss = loss_function(tar_real, predictions)\n",
    "\n",
    "    val_loss(loss)\n",
    "    val_accuracy(tar_real, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"transformer\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nencoder (Encoder)            multiple                  1831808   \n_________________________________________________________________\ndecoder (Decoder)            multiple                  1596800   \n_________________________________________________________________\ndense_64 (Dense)             multiple                  542703    \n=================================================================\nTotal params: 3,971,311\nTrainable params: 3,971,311\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "(inp, tar) = next(iter(train_dataset))\n",
    "train_step(inp, tar)\n",
    "\n",
    "transformer.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "run_id = f\"{num_layers}layers_{d_model}d_{num_heads}heads_{dff}dff\"\n",
    "checkpoint_path = os.path.join(checkpoint_path, run_id)\n",
    "log_dir = os.path.join(log_dir, run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Read checkpoint, 29 epoches have beed trained.\nWARNING:tensorflow:Trace already enabled\nWARNING:tensorflow:Trace already enabled\n"
    }
   ],
   "source": [
    "ckpt = tf.train.Checkpoint(model=transformer, optimizer=optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    last_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1]) * 5\n",
    "    print(f'Read checkpoint, {last_epoch - 1} epoches have beed trained.')\n",
    "else:\n",
    "    last_epoch = 0\n",
    "    print('Checkpoint not found.')\n",
    "\n",
    "summary_writer = tf.summary.create_file_writer(log_dir)\n",
    "tf.summary.trace_on(graph=True, profiler=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "478/478 [==============================] - 196s 411ms/step\nEpoch 30         Loss 0.7710         Accuracy 0.2621         Val_Loss 0.9092         Val_Accuracy 0.2515\nTime taken for 1 epoch: 204.60754680633545 secs\n\n478/478 [==============================] - 213s 446ms/step\nEpoch 31         Loss 0.7650         Accuracy 0.2631         Val_Loss 0.9055         Val_Accuracy 0.2528\nTime taken for 1 epoch: 217.48237371444702 secs\n\n478/478 [==============================] - 230s 481ms/step\nEpoch 32         Loss 0.7595         Accuracy 0.2638         Val_Loss 0.9048         Val_Accuracy 0.2529\nTime taken for 1 epoch: 234.59940481185913 secs\n\n478/478 [==============================] - 228s 478ms/step\nEpoch 33         Loss 0.7546         Accuracy 0.2646         Val_Loss 0.9045         Val_Accuracy 0.2528\nTime taken for 1 epoch: 232.9268627166748 secs\n\n478/478 [==============================] - 228s 478ms/step\nSaving checkpoint for epoch 34 at nmt\\checkpoints\\4layers_128d_8heads_512dff\\ckpt-7\nEpoch 34         Loss 0.7495         Accuracy 0.2655         Val_Loss 0.9065         Val_Accuracy 0.2528\nTime taken for 1 epoch: 233.20344400405884 secs\n\n478/478 [==============================] - 229s 479ms/step\nEpoch 35         Loss 0.7452         Accuracy 0.2662         Val_Loss 0.9027         Val_Accuracy 0.2536\nTime taken for 1 epoch: 233.5184998512268 secs\n\n478/478 [==============================] - 218s 457ms/step\nEpoch 36         Loss 0.7411         Accuracy 0.2668         Val_Loss 0.9027         Val_Accuracy 0.2541\nTime taken for 1 epoch: 221.56365180015564 secs\n\n478/478 [==============================] - 176s 368ms/step\nEpoch 37         Loss 0.7361         Accuracy 0.2674         Val_Loss 0.8988         Val_Accuracy 0.2540\nTime taken for 1 epoch: 178.99273490905762 secs\n\n478/478 [==============================] - 168s 351ms/step\nEpoch 38         Loss 0.7324         Accuracy 0.2681         Val_Loss 0.8987         Val_Accuracy 0.2541\nTime taken for 1 epoch: 171.06401991844177 secs\n\n478/478 [==============================] - 169s 353ms/step\nSaving checkpoint for epoch 39 at nmt\\checkpoints\\4layers_128d_8heads_512dff\\ckpt-8\nEpoch 39         Loss 0.7282         Accuracy 0.2687         Val_Loss 0.9024         Val_Accuracy 0.2541\nTime taken for 1 epoch: 172.2634675502777 secs\n\n"
    }
   ],
   "source": [
    "EPOCHS = 40\n",
    "\n",
    "for epoch in range(last_epoch, EPOCHS):\n",
    "    start = time.time()\n",
    "  \n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    val_loss.reset_states()\n",
    "    val_accuracy.reset_states()\n",
    "    \n",
    "    probar = tf.keras.utils.Progbar(478)\n",
    "    for (step_idx, (inp, tar)) in enumerate(train_dataset):\n",
    "        train_step(inp, tar)\n",
    "        probar.add(1)\n",
    "\n",
    "    for (step_idx, (inp, tar)) in enumerate(val_dataset):\n",
    "        val_step(inp, tar)\n",
    "\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "      ckpt_save_path = ckpt_manager.save()\n",
    "      print ('Saving checkpoint for epoch {} at {}'.format(epoch, ckpt_save_path))\n",
    "\n",
    "    with summary_writer.as_default():\n",
    "      tf.summary.scalar(\"train_loss\", train_loss.result(), step=epoch)\n",
    "      tf.summary.scalar(\"train_acc\", train_accuracy.result(), step=epoch)\n",
    "      tf.summary.scalar(\"val_loss\", val_loss.result(), step=epoch)\n",
    "      tf.summary.scalar(\"val_acc\", val_accuracy.result(), step=epoch)\n",
    "    \n",
    "    print(f'Epoch {epoch}\\\n",
    "         Loss {train_loss.result():.4f}\\\n",
    "         Accuracy {train_accuracy.result():.4f}\\\n",
    "         Val_Loss {val_loss.result():.4f}\\\n",
    "         Val_Accuracy {val_accuracy.result():.4f}')\n",
    "    \n",
    "    print('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
   ]
  }
 ]
}